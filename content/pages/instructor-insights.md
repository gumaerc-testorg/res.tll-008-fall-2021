---
content_type: page
layout: instructor_insights
learning_resource_types: []
ocw_type: ThisCourseAtMITSection
title: Instructor Insights
uid: ea6634c6-1841-f34c-97bd-e95620a88401
---

Instructor Insights
-------------------

{{< quote "The goal is to get students to think about data as the product of a long and complex process that is driven by human judgments and values." "— Harini Suresh" >}}

When computer science was in its infancy, programmers quickly realized that though computers are astonishingly powerful tools, the results they achieve are only as good as the data you feed into them. (This principle was quickly formalized as GIGO: “Garbage In, Garbage Out.”) What was true in the era of the UNIVAC has proved still to be true in the era of machine learning: among other well-publicized AI fiascos, chatbots that have interacted with bigots have learned to spew racist invective, while facial-recognition software trained solely on images of white people sometimes fails to recognize people of color as human. In 2021, Professor Catherine D’Ignazio of MIT’s Department of Urban Studies and Planning (DUSP) and Professor Jacob Andreas and Harini Suresh of the Department of Electrical Engineering and Computer Science collaborated as part of the Social and Ethical Responsibilities of Computing initiative to teach computer science students to recognize how deep learning systems can replicate and magnify the biases inherent in the data sets that are used to train them.

In the episode of the Chalk Radio podcast embedded below, Prof. D’Ignazio, Prof. Andreas, and graduate student Harini Suresh describe the principles behind their revised curriculum for the course 6.864 Quantitative Methods for Natural Language Processing.

{{< simplecast ecf4e6d6-8bd8-4a86-a052-d0374fe792bd >}}